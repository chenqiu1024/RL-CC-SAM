{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# nnU-Net Baseline Test - Complete Pipeline\n",
        "\n",
        "This notebook demonstrates a complete nnU-Net baseline test including:\n",
        "- 🔧 Environment setup and verification\n",
        "- 📊 Dataset preparation and preprocessing\n",
        "- 🏋️ Model training (quick baseline)\n",
        "- 🔮 Inference on test data\n",
        "- 📈 Results visualization and analysis\n",
        "- 🧹 Cleanup and teardown\n",
        "\n",
        "**Prerequisites:**\n",
        "- nnU-Net installed as submodule (ref: nnunet/readme.md)\n",
        "- Conda environment activated\n",
        "- Required dependencies available\n",
        "\n",
        "**References:**\n",
        "- nnU-Net documentation: nnunet/documentation/how_to_use_nnunet.md\n",
        "- Dataset format: nnunet/documentation/dataset_format.md\n",
        "- Training guide: nnunet/documentation/training_a_new_model.md\n",
        "\n",
        "**Estimated Runtime:** 30-60 minutes (depending on dataset size and hardware)\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 🔧 1. Environment Setup and Verification\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Environment setup and Colab detection\n",
        "# Ref: setup_colab.ipynb for Colab integration patterns\n",
        "import sys\n",
        "import os\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "if IN_COLAB:\n",
        "    print(\"🌐 Running in Google Colab\")\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    \n",
        "    # Set up paths for Colab - adjust these paths as needed\n",
        "    DRIVE_ROOT = '/content/drive/MyDrive'\n",
        "    PROJECT_ROOT = f'{DRIVE_ROOT}/RL-CC-SAM'\n",
        "    \n",
        "    # Change to project directory\n",
        "    os.chdir(PROJECT_ROOT)\n",
        "    sys.path.append(PROJECT_ROOT)\n",
        "    \n",
        "    print(f\"📁 Working directory: {os.getcwd()}\")\n",
        "else:\n",
        "    print(\"💻 Running locally\")\n",
        "    # Assume notebook is in notebooks/ folder\n",
        "    PROJECT_ROOT = Path.cwd().parent\n",
        "    os.chdir(PROJECT_ROOT)\n",
        "    \n",
        "    print(f\"📁 Working directory: {PROJECT_ROOT}\")\n",
        "\n",
        "# Create experiment directory with timestamp\n",
        "# Following pattern from DATASET_SETUP_README.md\n",
        "EXPERIMENT_ID = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "EXPERIMENT_DIR = Path(\"notebooks\") / f\"baseline_test_{EXPERIMENT_ID}\"\n",
        "EXPERIMENT_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "print(f\"🧪 Experiment ID: {EXPERIMENT_ID}\")\n",
        "print(f\"📁 Experiment directory: {EXPERIMENT_DIR}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required dependencies\n",
        "# Ref: download_requirements.txt for required packages\n",
        "%pip install -q requests tqdm nibabel matplotlib seaborn plotly scipy ipywidgets\n",
        "\n",
        "# Import essential libraries\n",
        "import json\n",
        "import time\n",
        "import shutil\n",
        "import tempfile\n",
        "import subprocess\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Data handling\n",
        "import numpy as np\n",
        "import nibabel as nib\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from matplotlib.patches import Rectangle\n",
        "\n",
        "# Set style\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"✅ Dependencies imported successfully\")\n",
        "print(f\"📊 NumPy version: {np.__version__}\")\n",
        "print(f\"🧠 NiBabel version: {nib.__version__}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup nnU-Net environment variables and paths\n",
        "# Ref: setup_nnunet_env.sh and DATASET_SETUP_README.md lines 45-54\n",
        "\n",
        "# Define paths\n",
        "PROJECT_ROOT = Path.cwd()\n",
        "DATASETS_DIR = PROJECT_ROOT / \"datasets\"\n",
        "NNUNET_DIR = PROJECT_ROOT / \"nnunet\"\n",
        "\n",
        "print(f\"📁 Project root: {PROJECT_ROOT}\")\n",
        "print(f\"📁 Datasets directory: {DATASETS_DIR}\")\n",
        "print(f\"📁 nnU-Net submodule: {NNUNET_DIR}\")\n",
        "\n",
        "# Set nnU-Net environment variables\n",
        "# Following nnunet/documentation/installation_instructions.md section 3\n",
        "env_vars = {\n",
        "    'nnUNet_raw': str(DATASETS_DIR / \"nnUNet_raw\"),\n",
        "    'nnUNet_preprocessed': str(DATASETS_DIR / \"nnUNet_preprocessed\"),\n",
        "    'nnUNet_results': str(DATASETS_DIR / \"nnUNet_results\"),\n",
        "    'EXPERIMENT_DIR': str(EXPERIMENT_DIR)\n",
        "}\n",
        "\n",
        "# Set environment variables and create directories\n",
        "for key, value in env_vars.items():\n",
        "    os.environ[key] = value\n",
        "    if key.startswith('nnUNet'):\n",
        "        Path(value).mkdir(parents=True, exist_ok=True)\n",
        "    print(f\"✅ {key}: {value}\")\n",
        "\n",
        "# Verify nnU-Net installation\n",
        "# Following test_nnunet_setup.py pattern\n",
        "try:\n",
        "    result = subprocess.run(['nnUNetv2_train', '-h'], \n",
        "                          capture_output=True, text=True, timeout=10)\n",
        "    if result.returncode == 0:\n",
        "        print(\"✅ nnU-Net commands available\")\n",
        "    else:\n",
        "        print(\"⚠️ nnU-Net commands may not be properly installed\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ nnU-Net verification failed: {e}\")\n",
        "    print(\"💡 Make sure nnU-Net is installed: pip install nnunetv2\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 📊 2. Dataset Preparation\n",
        "\n",
        "We'll create a synthetic 3D medical dataset for baseline testing. This follows the nnU-Net dataset format specification from `nnunet/documentation/dataset_format.md`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dataset configuration\n",
        "# Following nnunet/documentation/dataset_format.md for naming conventions\n",
        "DATASET_ID = 999  # Using 999 for synthetic/test datasets\n",
        "DATASET_NAME = f\"Dataset{DATASET_ID:03d}_BaselineTest\"\n",
        "DATASET_PATH = Path(os.environ['nnUNet_raw']) / DATASET_NAME\n",
        "\n",
        "print(f\"🏷️ Dataset ID: {DATASET_ID}\")\n",
        "print(f\"📁 Dataset path: {DATASET_PATH}\")\n",
        "\n",
        "def create_synthetic_medical_dataset(dataset_path, num_train=15, num_test=3):\n",
        "    \"\"\"\n",
        "    Create a synthetic 3D medical dataset for baseline testing.\n",
        "    \n",
        "    This creates realistic-looking medical data following the nnU-Net format:\n",
        "    - imagesTr/: Training images (case_XXX_0000.nii.gz)\n",
        "    - labelsTr/: Training labels (case_XXX.nii.gz)  \n",
        "    - imagesTs/: Test images (test_XXX_0000.nii.gz)\n",
        "    - dataset.json: Dataset metadata\n",
        "    \n",
        "    Ref: nnunet/documentation/dataset_format.md lines 1-50\n",
        "    \"\"\"\n",
        "    print(f\"🔬 Creating synthetic dataset: {num_train} training + {num_test} test cases\")\n",
        "    \n",
        "    # Create directory structure per nnU-Net specification\n",
        "    dataset_path = Path(dataset_path)\n",
        "    imagesTr = dataset_path / \"imagesTr\"\n",
        "    labelsTr = dataset_path / \"labelsTr\"\n",
        "    imagesTs = dataset_path / \"imagesTs\"\n",
        "    \n",
        "    for dir_path in [imagesTr, labelsTr, imagesTs]:\n",
        "        dir_path.mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "    # Dataset parameters - realistic medical image dimensions\n",
        "    img_shape = (96, 96, 48)  # X, Y, Z dimensions\n",
        "    spacing = [1.5, 1.5, 3.0]  # Typical medical image spacing (mm)\n",
        "    affine = np.eye(4)\n",
        "    np.fill_diagonal(affine[:3, :3], spacing)\n",
        "    \n",
        "    print(f\"   📐 Image dimensions: {img_shape}\")\n",
        "    print(f\"   📏 Voxel spacing: {spacing} mm\")\n",
        "    \n",
        "    # Create training data\n",
        "    for i in tqdm(range(num_train), desc=\"Creating training data\"):\n",
        "        case_id = f\"case_{i:03d}\"\n",
        "        \n",
        "        # Generate realistic medical image with tissue contrast\n",
        "        # Background: low intensity with noise (air/background)\n",
        "        image = np.random.normal(100, 30, img_shape).astype(np.float32)\n",
        "        \n",
        "        # Add anatomical structure (heart-like organ)\n",
        "        center = np.array(img_shape) // 2\n",
        "        organ_mask = np.zeros(img_shape, dtype=bool)\n",
        "        \n",
        "        # Create ellipsoid organ using standard equation\n",
        "        for z in range(img_shape[2]):\n",
        "            for y in range(img_shape[1]):\n",
        "                for x in range(img_shape[0]):\n",
        "                    # Ellipsoid: (x-cx)²/a² + (y-cy)²/b² + (z-cz)²/c² ≤ 1\n",
        "                    dx = (x - center[0]) / 20\n",
        "                    dy = (y - center[1]) / 25\n",
        "                    dz = (z - center[2]) / 15\n",
        "                    \n",
        "                    if dx**2 + dy**2 + dz**2 <= 1:\n",
        "                        organ_mask[x, y, z] = True\n",
        "        \n",
        "        # Set organ intensity (soft tissue HU values)\n",
        "        image[organ_mask] = np.random.normal(300, 50, np.sum(organ_mask))\n",
        "        \n",
        "        # Add small lesions (pathological findings)\n",
        "        lesion_mask = np.zeros(img_shape, dtype=bool)\n",
        "        num_lesions = np.random.randint(1, 4)\n",
        "        \n",
        "        for _ in range(num_lesions):\n",
        "            # Random position within organ\n",
        "            organ_coords = np.where(organ_mask)\n",
        "            if len(organ_coords[0]) > 0:\n",
        "                idx = np.random.randint(len(organ_coords[0]))\n",
        "                lx, ly, lz = organ_coords[0][idx], organ_coords[1][idx], organ_coords[2][idx]\n",
        "                \n",
        "                # Small spherical lesion (radius ~3 voxels)\n",
        "                for dx in range(-3, 4):\n",
        "                    for dy in range(-3, 4):\n",
        "                        for dz in range(-2, 3):\n",
        "                            x, y, z = lx + dx, ly + dy, lz + dz\n",
        "                            if (0 <= x < img_shape[0] and 0 <= y < img_shape[1] and \n",
        "                                0 <= z < img_shape[2] and dx**2 + dy**2 + dz**2 <= 9):\n",
        "                                lesion_mask[x, y, z] = True\n",
        "                                image[x, y, z] = np.random.normal(500, 30)\n",
        "        \n",
        "        # Create segmentation mask with proper labels\n",
        "        # Following nnU-Net label convention: 0=background, 1=organ, 2=lesion\n",
        "        seg_mask = np.zeros(img_shape, dtype=np.uint8)\n",
        "        seg_mask[organ_mask] = 1  # Organ label\n",
        "        seg_mask[lesion_mask] = 2  # Lesion label\n",
        "        \n",
        "        # Add realistic blur and noise\n",
        "        from scipy import ndimage\n",
        "        image = ndimage.gaussian_filter(image, sigma=0.5)\n",
        "        image = np.clip(image, 0, 1000)  # Realistic HU range\n",
        "        \n",
        "        # Save as NIfTI files with proper naming convention\n",
        "        # Ref: nnunet/documentation/dataset_format.md lines 15-25\n",
        "        img_nifti = nib.Nifti1Image(image.astype(np.int16), affine)\n",
        "        seg_nifti = nib.Nifti1Image(seg_mask, affine)\n",
        "        \n",
        "        nib.save(img_nifti, imagesTr / f\"{case_id}_0000.nii.gz\")\n",
        "        nib.save(seg_nifti, labelsTr / f\"{case_id}.nii.gz\")\n",
        "    \n",
        "    # Create test data (images only, no labels)\n",
        "    test_cases = []\n",
        "    for i in tqdm(range(num_test), desc=\"Creating test data\"):\n",
        "        case_id = f\"test_{i:03d}\"\n",
        "        test_cases.append(case_id)\n",
        "        \n",
        "        # Similar to training data but with slight variations\n",
        "        image = np.random.normal(100, 30, img_shape).astype(np.float32)\n",
        "        \n",
        "        # Add organ structure with slight shape variation\n",
        "        center = np.array(img_shape) // 2 + np.random.randint(-5, 6, 3)\n",
        "        organ_mask = np.zeros(img_shape, dtype=bool)\n",
        "        \n",
        "        for z in range(img_shape[2]):\n",
        "            for y in range(img_shape[1]):\n",
        "                for x in range(img_shape[0]):\n",
        "                    # Slightly different organ shape\n",
        "                    dx = (x - center[0]) / (18 + np.random.normal(0, 2))\n",
        "                    dy = (y - center[1]) / (23 + np.random.normal(0, 2))\n",
        "                    dz = (z - center[2]) / (13 + np.random.normal(0, 2))\n",
        "                    \n",
        "                    if dx**2 + dy**2 + dz**2 <= 1:\n",
        "                        organ_mask[x, y, z] = True\n",
        "        \n",
        "        image[organ_mask] = np.random.normal(300, 50, np.sum(organ_mask))\n",
        "        \n",
        "        # Add blur and save\n",
        "        image = ndimage.gaussian_filter(image, sigma=0.5)\n",
        "        image = np.clip(image, 0, 1000)\n",
        "        \n",
        "        img_nifti = nib.Nifti1Image(image.astype(np.int16), affine)\n",
        "        nib.save(img_nifti, imagesTs / f\"{case_id}_0000.nii.gz\")\n",
        "    \n",
        "    # Create dataset.json with metadata\n",
        "    # Following nnunet/documentation/dataset_format.md lines 60-100\n",
        "    dataset_json = {\n",
        "        \"name\": \"BaselineTest\",\n",
        "        \"description\": \"Synthetic dataset for nnU-Net baseline testing\",\n",
        "        \"tensorImageSize\": \"3D\",\n",
        "        \"reference\": \"Created for baseline testing\",\n",
        "        \"licence\": \"For testing purposes only\",\n",
        "        \"release\": \"1.0\",\n",
        "        \"modality\": {\"0\": \"CT\"},  # Single modality\n",
        "        \"labels\": {\n",
        "            \"0\": \"Background\",\n",
        "            \"1\": \"Organ\", \n",
        "            \"2\": \"Lesion\"\n",
        "        },\n",
        "        \"numTraining\": num_train,\n",
        "        \"numTest\": num_test,\n",
        "        \"training\": [{\n",
        "            \"image\": f\"./imagesTr/case_{i:03d}_0000.nii.gz\",\n",
        "            \"label\": f\"./labelsTr/case_{i:03d}.nii.gz\"\n",
        "        } for i in range(num_train)],\n",
        "        \"test\": [f\"./imagesTs/{case_id}_0000.nii.gz\" for case_id in test_cases]\n",
        "    }\n",
        "    \n",
        "    with open(dataset_path / \"dataset.json\", 'w') as f:\n",
        "        json.dump(dataset_json, f, indent=2)\n",
        "    \n",
        "    print(f\"✅ Synthetic dataset created successfully!\")\n",
        "    print(f\"   📊 Training cases: {num_train}\")\n",
        "    print(f\"   🧪 Test cases: {num_test}\")\n",
        "    print(f\"   📐 Image shape: {img_shape}\")\n",
        "    print(f\"   🏷️ Labels: Background (0), Organ (1), Lesion (2)\")\n",
        "    \n",
        "    return dataset_path\n",
        "\n",
        "# Create the synthetic dataset\n",
        "created_dataset = create_synthetic_medical_dataset(DATASET_PATH, num_train=15, num_test=3)\n",
        "print(f\"\\n📁 Dataset created at: {created_dataset}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize sample data from the created dataset\n",
        "def visualize_sample_data(dataset_path, case_idx=0):\n",
        "    \"\"\"Visualize a sample case from the dataset.\"\"\"\n",
        "    \n",
        "    dataset_path = Path(dataset_path)\n",
        "    case_id = f\"case_{case_idx:03d}\"\n",
        "    \n",
        "    # Load image and label\n",
        "    img_path = dataset_path / \"imagesTr\" / f\"{case_id}_0000.nii.gz\"\n",
        "    label_path = dataset_path / \"labelsTr\" / f\"{case_id}.nii.gz\"\n",
        "    \n",
        "    if not img_path.exists() or not label_path.exists():\n",
        "        print(f\"❌ Files not found for case {case_id}\")\n",
        "        return\n",
        "    \n",
        "    # Load NIfTI files\n",
        "    img_nii = nib.load(img_path)\n",
        "    label_nii = nib.load(label_path)\n",
        "    \n",
        "    img_data = img_nii.get_fdata()\n",
        "    label_data = label_nii.get_fdata()\n",
        "    \n",
        "    print(f\"📊 Sample Case: {case_id}\")\n",
        "    print(f\"   Image shape: {img_data.shape}\")\n",
        "    print(f\"   Image range: [{img_data.min():.1f}, {img_data.max():.1f}]\")\n",
        "    print(f\"   Label shape: {label_data.shape}\")\n",
        "    print(f\"   Unique labels: {np.unique(label_data)}\")\n",
        "    print(f\"   Voxel spacing: {img_nii.header.get_zooms()[:3]}\")\n",
        "    \n",
        "    # Create visualization - 3 orthogonal views\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
        "    \n",
        "    # Get middle slices\n",
        "    mid_x, mid_y, mid_z = [s // 2 for s in img_data.shape]\n",
        "    \n",
        "    # Axial view (XY plane)\n",
        "    axes[0, 0].imshow(img_data[:, :, mid_z].T, cmap='gray', origin='lower')\n",
        "    axes[0, 0].set_title(f'Image - Axial (Z={mid_z})')\n",
        "    axes[0, 0].axis('off')\n",
        "    \n",
        "    axes[1, 0].imshow(label_data[:, :, mid_z].T, cmap='viridis', origin='lower')\n",
        "    axes[1, 0].set_title(f'Label - Axial (Z={mid_z})')\n",
        "    axes[1, 0].axis('off')\n",
        "    \n",
        "    # Sagittal view (YZ plane)\n",
        "    axes[0, 1].imshow(img_data[mid_x, :, :].T, cmap='gray', origin='lower')\n",
        "    axes[0, 1].set_title(f'Image - Sagittal (X={mid_x})')\n",
        "    axes[0, 1].axis('off')\n",
        "    \n",
        "    axes[1, 1].imshow(label_data[mid_x, :, :].T, cmap='viridis', origin='lower')\n",
        "    axes[1, 1].set_title(f'Label - Sagittal (X={mid_x})')\n",
        "    axes[1, 1].axis('off')\n",
        "    \n",
        "    # Coronal view (XZ plane)\n",
        "    axes[0, 2].imshow(img_data[:, mid_y, :].T, cmap='gray', origin='lower')\n",
        "    axes[0, 2].set_title(f'Image - Coronal (Y={mid_y})')\n",
        "    axes[0, 2].axis('off')\n",
        "    \n",
        "    axes[1, 2].imshow(label_data[:, mid_y, :].T, cmap='viridis', origin='lower')\n",
        "    axes[1, 2].set_title(f'Label - Coronal (Y={mid_y})')\n",
        "    axes[1, 2].axis('off')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig(EXPERIMENT_DIR / f'sample_data_{case_id}.png', dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    \n",
        "    # Print label statistics\n",
        "    print(f\"\\n📈 Label Statistics:\")\n",
        "    for label_val in np.unique(label_data):\n",
        "        count = np.sum(label_data == label_val)\n",
        "        percentage = (count / label_data.size) * 100\n",
        "        label_names = {0: 'Background', 1: 'Organ', 2: 'Lesion'}\n",
        "        name = label_names.get(int(label_val), f'Unknown({int(label_val)})')\n",
        "        print(f\"   {name}: {count:,} voxels ({percentage:.1f}%)\")\n",
        "\n",
        "# Visualize first case\n",
        "visualize_sample_data(DATASET_PATH, case_idx=0)\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 🏋️ 3. Data Preprocessing and Model Training\n",
        "\n",
        "nnU-Net requires preprocessing before training. This step analyzes the dataset and determines optimal preprocessing parameters.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run nnU-Net preprocessing\n",
        "# Ref: nnunet/documentation/how_to_use_nnunet.md lines 100-120\n",
        "print(f\"🔄 Starting nnU-Net preprocessing for dataset {DATASET_ID}...\")\n",
        "print(f\"📁 Raw data: {os.environ['nnUNet_raw']}\")\n",
        "print(f\"📁 Preprocessed: {os.environ['nnUNet_preprocessed']}\")\n",
        "\n",
        "# Run preprocessing command\n",
        "# Following DATASET_SETUP_README.md lines 80-90\n",
        "preprocessing_cmd = f\"nnUNetv2_plan_and_preprocess -d {DATASET_ID} -c 2d 3d_fullres --verify_dataset_integrity\"\n",
        "print(f\"\\n🚀 Running command: {preprocessing_cmd}\")\n",
        "\n",
        "start_time = time.time()\n",
        "try:\n",
        "    result = subprocess.run(preprocessing_cmd.split(), \n",
        "                          capture_output=True, text=True, timeout=600)  # 10 min timeout\n",
        "    \n",
        "    preprocessing_time = time.time() - start_time\n",
        "    \n",
        "    if result.returncode == 0:\n",
        "        print(f\"✅ Preprocessing completed successfully in {preprocessing_time:.1f}s\")\n",
        "        print(\"\\n📋 Preprocessing output (last 1000 chars):\")\n",
        "        print(result.stdout[-1000:])\n",
        "    else:\n",
        "        print(f\"❌ Preprocessing failed with return code: {result.returncode}\")\n",
        "        print(\"\\nError output:\")\n",
        "        print(result.stderr)\n",
        "        \n",
        "except subprocess.TimeoutExpired:\n",
        "    print(\"⏰ Preprocessing timed out after 10 minutes\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Error during preprocessing: {e}\")\n",
        "\n",
        "# Check if preprocessing files were created\n",
        "preprocessed_path = Path(os.environ['nnUNet_preprocessed']) / f\"Dataset{DATASET_ID:03d}_BaselineTest\"\n",
        "if preprocessed_path.exists():\n",
        "    print(f\"\\n✅ Preprocessed dataset found at: {preprocessed_path}\")\n",
        "    \n",
        "    # List key files\n",
        "    print(\"\\n📁 Key preprocessed files:\")\n",
        "    for pattern in [\"*.json\", \"*.pkl\"]:\n",
        "        for item in preprocessed_path.glob(pattern):\n",
        "            size = item.stat().st_size / 1024  # KB\n",
        "            print(f\"   {item.name} ({size:.1f} KB)\")\n",
        "else:\n",
        "    print(f\"❌ Preprocessed dataset not found at: {preprocessed_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Quick baseline training (2D, single fold)\n",
        "# Ref: nnunet/documentation/training_a_new_model.md lines 1-50\n",
        "print(f\"🏋️ Starting baseline training...\")\n",
        "print(f\"   Dataset: {DATASET_ID}\")\n",
        "print(f\"   Configuration: 2d (fast training)\")\n",
        "print(f\"   Fold: 0 (single fold for quick testing)\")\n",
        "print(f\"   Expected time: 5-15 minutes\")\n",
        "\n",
        "# Training command with memory optimization\n",
        "# Following DATASET_SETUP_README.md lines 150-160\n",
        "training_cmd = f\"nnUNetv2_train {DATASET_ID} 2d 0 --npz\"  # --npz saves memory\n",
        "print(f\"\\n🚀 Training command: {training_cmd}\")\n",
        "\n",
        "# Create training log file\n",
        "training_log = EXPERIMENT_DIR / \"training_log.txt\"\n",
        "print(f\"📝 Training log will be saved to: {training_log}\")\n",
        "print(\"\\n⏳ Training started... This may take several minutes.\")\n",
        "\n",
        "training_start_time = time.time()\n",
        "\n",
        "try:\n",
        "    # Run training with live output capture\n",
        "    process = subprocess.Popen(training_cmd.split(), \n",
        "                              stdout=subprocess.PIPE, \n",
        "                              stderr=subprocess.STDOUT,\n",
        "                              universal_newlines=True,\n",
        "                              bufsize=1)\n",
        "    \n",
        "    # Stream output to both console and file\n",
        "    output_lines = []\n",
        "    with open(training_log, 'w') as log_file:\n",
        "        while True:\n",
        "            output = process.stdout.readline()\n",
        "            if output == '' and process.poll() is not None:\n",
        "                break\n",
        "            if output:\n",
        "                output_lines.append(output.strip())\n",
        "                # Show last few lines to avoid overwhelming output\n",
        "                if len(output_lines) % 10 == 0:\n",
        "                    print(f\"  ... {output.strip()}\")\n",
        "                log_file.write(output)\n",
        "                log_file.flush()\n",
        "    \n",
        "    return_code = process.poll()\n",
        "    training_time = time.time() - training_start_time\n",
        "    \n",
        "    if return_code == 0:\n",
        "        print(f\"\\n✅ Training completed successfully in {training_time/60:.1f} minutes!\")\n",
        "        \n",
        "        # Check if model files were created\n",
        "        model_path = Path(os.environ['nnUNet_results']) / f\"Dataset{DATASET_ID:03d}_BaselineTest\" / \"nnUNetTrainer__nnUNetPlans__2d\" / \"fold_0\"\n",
        "        \n",
        "        if model_path.exists():\n",
        "            print(f\"\\n🎯 Model saved at: {model_path}\")\n",
        "            \n",
        "            # List model files\n",
        "            model_files = list(model_path.glob(\"*.pth\"))\n",
        "            print(f\"\\n📦 Model files created:\")\n",
        "            for model_file in model_files:\n",
        "                size = model_file.stat().st_size / (1024*1024)  # MB\n",
        "                print(f\"   {model_file.name} ({size:.1f} MB)\")\n",
        "        else:\n",
        "            print(f\"⚠️ Model directory not found at: {model_path}\")\n",
        "            \n",
        "    else:\n",
        "        print(f\"❌ Training failed with return code: {return_code}\")\n",
        "        print(f\"📝 Check training log for details: {training_log}\")\n",
        "        \n",
        "except KeyboardInterrupt:\n",
        "    print(\"\\n⏹️ Training interrupted by user\")\n",
        "    process.terminate()\n",
        "except Exception as e:\n",
        "    print(f\"\\n❌ Error during training: {e}\")\n",
        "    print(f\"📝 Check training log for details: {training_log}\")\n",
        "\n",
        "print(f\"\\n📊 Training Summary:\")\n",
        "print(f\"   Duration: {(time.time() - training_start_time)/60:.1f} minutes\")\n",
        "print(f\"   Log file: {training_log}\")\n",
        "print(f\"   Dataset: {DATASET_ID} (2D configuration)\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 🔮 4. Model Inference\n",
        "\n",
        "Now we'll run inference on the test data using the trained model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup inference directories\n",
        "INPUT_DIR = EXPERIMENT_DIR / \"inference_input\"\n",
        "OUTPUT_DIR = EXPERIMENT_DIR / \"inference_output\"\n",
        "\n",
        "# Create directories\n",
        "INPUT_DIR.mkdir(exist_ok=True)\n",
        "OUTPUT_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "print(f\"🔮 Setting up inference...\")\n",
        "print(f\"   Input directory: {INPUT_DIR}\")\n",
        "print(f\"   Output directory: {OUTPUT_DIR}\")\n",
        "\n",
        "# Copy test images to inference input directory\n",
        "test_images_dir = DATASET_PATH / \"imagesTs\"\n",
        "test_images = list(test_images_dir.glob(\"*.nii.gz\"))\n",
        "\n",
        "print(f\"\\n📁 Found {len(test_images)} test images\")\n",
        "\n",
        "# Copy test images to inference directory\n",
        "for test_img in test_images:\n",
        "    shutil.copy2(test_img, INPUT_DIR)\n",
        "    print(f\"   Copied: {test_img.name}\")\n",
        "\n",
        "# Run inference\n",
        "# Ref: nnunet/documentation/how_to_use_nnunet.md lines 200-220\n",
        "inference_cmd = f\"nnUNetv2_predict -i {INPUT_DIR} -o {OUTPUT_DIR} -d {DATASET_ID} -c 2d -f 0\"\n",
        "print(f\"\\n🚀 Inference command: {inference_cmd}\")\n",
        "\n",
        "inference_start_time = time.time()\n",
        "\n",
        "try:\n",
        "    result = subprocess.run(inference_cmd.split(), \n",
        "                          capture_output=True, text=True, timeout=300)  # 5 min timeout\n",
        "    \n",
        "    inference_time = time.time() - inference_start_time\n",
        "    \n",
        "    if result.returncode == 0:\n",
        "        print(f\"✅ Inference completed successfully in {inference_time:.1f}s\")\n",
        "        \n",
        "        # List output files\n",
        "        output_files = list(OUTPUT_DIR.glob(\"*.nii.gz\"))\n",
        "        print(f\"\\n📊 Generated {len(output_files)} prediction files:\")\n",
        "        \n",
        "        for output_file in output_files:\n",
        "            size = output_file.stat().st_size / (1024*1024)  # MB\n",
        "            print(f\"   {output_file.name} ({size:.2f} MB)\")\n",
        "            \n",
        "    else:\n",
        "        print(f\"❌ Inference failed with return code: {result.returncode}\")\n",
        "        print(\"\\nError output:\")\n",
        "        print(result.stderr)\n",
        "        \n",
        "except subprocess.TimeoutExpired:\n",
        "    print(\"⏰ Inference timed out after 5 minutes\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Error during inference: {e}\")\n",
        "\n",
        "print(f\"\\n🔮 Inference Summary:\")\n",
        "print(f\"   Duration: {inference_time:.1f}s\")\n",
        "print(f\"   Input files: {len(test_images)}\")\n",
        "print(f\"   Output directory: {OUTPUT_DIR}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 📈 5. Results Visualization and Analysis\n",
        "\n",
        "Let's analyze and visualize the prediction results.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze and visualize inference results\n",
        "def analyze_predictions(input_dir, output_dir, experiment_dir):\n",
        "    \"\"\"Analyze and visualize prediction results.\"\"\"\n",
        "    \n",
        "    input_files = list(Path(input_dir).glob(\"*.nii.gz\"))\n",
        "    output_files = list(Path(output_dir).glob(\"*.nii.gz\"))\n",
        "    \n",
        "    print(f\"📊 Analyzing {len(output_files)} predictions...\")\n",
        "    \n",
        "    results_summary = []\n",
        "    \n",
        "    for i, (input_file, output_file) in enumerate(zip(sorted(input_files), sorted(output_files))):\n",
        "        print(f\"\\n🔍 Analyzing case {i+1}: {input_file.stem}\")\n",
        "        \n",
        "        # Load input image and prediction\n",
        "        try:\n",
        "            input_nii = nib.load(input_file)\n",
        "            pred_nii = nib.load(output_file)\n",
        "            \n",
        "            input_data = input_nii.get_fdata()\n",
        "            pred_data = pred_nii.get_fdata()\n",
        "            \n",
        "            # Basic statistics\n",
        "            unique_labels = np.unique(pred_data)\n",
        "            pred_stats = {}\n",
        "            \n",
        "            for label in unique_labels:\n",
        "                count = np.sum(pred_data == label)\n",
        "                percentage = (count / pred_data.size) * 100\n",
        "                pred_stats[int(label)] = {'count': count, 'percentage': percentage}\n",
        "            \n",
        "            # Store results\n",
        "            case_result = {\n",
        "                'case': input_file.stem,\n",
        "                'input_shape': input_data.shape,\n",
        "                'pred_shape': pred_data.shape,\n",
        "                'input_range': [float(input_data.min()), float(input_data.max())],\n",
        "                'predicted_labels': pred_stats\n",
        "            }\n",
        "            results_summary.append(case_result)\n",
        "            \n",
        "            print(f\"   Input shape: {input_data.shape}\")\n",
        "            print(f\"   Input range: [{input_data.min():.1f}, {input_data.max():.1f}]\")\n",
        "            print(f\"   Predicted labels: {list(unique_labels)}\")\n",
        "            \n",
        "            for label, stats in pred_stats.items():\n",
        "                label_names = {0: 'Background', 1: 'Organ', 2: 'Lesion'}\n",
        "                name = label_names.get(label, f'Label_{label}')\n",
        "                print(f\"     {name}: {stats['count']:,} voxels ({stats['percentage']:.1f}%)\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"   ❌ Error loading case: {e}\")\n",
        "            continue\n",
        "    \n",
        "    # Save results summary\n",
        "    results_file = Path(experiment_dir) / \"inference_results.json\"\n",
        "    with open(results_file, 'w') as f:\n",
        "        json.dump(results_summary, f, indent=2)\n",
        "    \n",
        "    print(f\"\\n💾 Results summary saved to: {results_file}\")\n",
        "    return results_summary\n",
        "\n",
        "# Analyze results\n",
        "results = analyze_predictions(INPUT_DIR, OUTPUT_DIR, EXPERIMENT_DIR)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize prediction results\n",
        "def visualize_predictions(input_dir, output_dir, experiment_dir, case_idx=0):\n",
        "    \"\"\"Create detailed visualizations of prediction results.\"\"\"\n",
        "    \n",
        "    input_files = sorted(list(Path(input_dir).glob(\"*.nii.gz\")))\n",
        "    output_files = sorted(list(Path(output_dir).glob(\"*.nii.gz\")))\n",
        "    \n",
        "    if case_idx >= len(input_files):\n",
        "        print(f\"❌ Case index {case_idx} out of range. Available: 0-{len(input_files)-1}\")\n",
        "        return\n",
        "    \n",
        "    input_file = input_files[case_idx]\n",
        "    output_file = output_files[case_idx]\n",
        "    \n",
        "    print(f\"📊 Visualizing case {case_idx}: {input_file.stem}\")\n",
        "    \n",
        "    # Load data\n",
        "    input_nii = nib.load(input_file)\n",
        "    pred_nii = nib.load(output_file)\n",
        "    \n",
        "    input_data = input_nii.get_fdata()\n",
        "    pred_data = pred_nii.get_fdata()\n",
        "    \n",
        "    # Create comprehensive visualization\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "    \n",
        "    # Get middle slices\n",
        "    mid_x, mid_y, mid_z = [s // 2 for s in input_data.shape]\n",
        "    \n",
        "    # Define colormap for predictions\n",
        "    from matplotlib.colors import ListedColormap\n",
        "    colors = ['black', 'red', 'yellow', 'cyan']\n",
        "    pred_cmap = ListedColormap(colors[:len(np.unique(pred_data))])\n",
        "    \n",
        "    # Axial view\n",
        "    axes[0, 0].imshow(input_data[:, :, mid_z].T, cmap='gray', origin='lower')\n",
        "    axes[0, 0].set_title(f'Input - Axial (Z={mid_z})')\n",
        "    axes[0, 0].axis('off')\n",
        "    \n",
        "    im1 = axes[1, 0].imshow(pred_data[:, :, mid_z].T, cmap=pred_cmap, origin='lower', vmin=0)\n",
        "    axes[1, 0].set_title(f'Prediction - Axial (Z={mid_z})')\n",
        "    axes[1, 0].axis('off')\n",
        "    \n",
        "    # Sagittal view\n",
        "    axes[0, 1].imshow(input_data[mid_x, :, :].T, cmap='gray', origin='lower')\n",
        "    axes[0, 1].set_title(f'Input - Sagittal (X={mid_x})')\n",
        "    axes[0, 1].axis('off')\n",
        "    \n",
        "    axes[1, 1].imshow(pred_data[mid_x, :, :].T, cmap=pred_cmap, origin='lower', vmin=0)\n",
        "    axes[1, 1].set_title(f'Prediction - Sagittal (X={mid_x})')\n",
        "    axes[1, 1].axis('off')\n",
        "    \n",
        "    # Coronal view\n",
        "    axes[0, 2].imshow(input_data[:, mid_y, :].T, cmap='gray', origin='lower')\n",
        "    axes[0, 2].set_title(f'Input - Coronal (Y={mid_y})')\n",
        "    axes[0, 2].axis('off')\n",
        "    \n",
        "    axes[1, 2].imshow(pred_data[:, mid_y, :].T, cmap=pred_cmap, origin='lower', vmin=0)\n",
        "    axes[1, 2].set_title(f'Prediction - Coronal (Y={mid_y})')\n",
        "    axes[1, 2].axis('off')\n",
        "    \n",
        "    # Add colorbar\n",
        "    cbar = plt.colorbar(im1, ax=axes[1, :], orientation='horizontal', fraction=0.02, pad=0.05)\n",
        "    cbar.set_label('Predicted Labels')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    \n",
        "    # Save visualization\n",
        "    vis_path = Path(experiment_dir) / f'prediction_visualization_case_{case_idx}.png'\n",
        "    plt.savefig(vis_path, dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    \n",
        "    print(f\"💾 Visualization saved to: {vis_path}\")\n",
        "    \n",
        "    # Print detailed statistics\n",
        "    print(f\"\\n📈 Detailed Statistics:\")\n",
        "    print(f\"   Input shape: {input_data.shape}\")\n",
        "    print(f\"   Input range: [{input_data.min():.1f}, {input_data.max():.1f}]\")\n",
        "    print(f\"   Prediction shape: {pred_data.shape}\")\n",
        "    print(f\"   Unique predictions: {np.unique(pred_data)}\")\n",
        "    \n",
        "         label_names = {0: 'Background', 1: 'Organ', 2: 'Lesion'}\n",
        "          for label in np.unique(pred_data):\n",
        "         count = np.sum(pred_data == label)\n",
        "         percentage = (count / pred_data.size) * 100\n",
        "         name = label_names.get(int(label), f'Label_{int(label)}')\n",
        "         print(f\"   {name}: {count:,} voxels ({percentage:.2f}%)\")\n",
        "\n",
        "# Visualize first prediction\n",
        "if len(list(OUTPUT_DIR.glob(\"*.nii.gz\"))) > 0:\n",
        "    visualize_predictions(INPUT_DIR, OUTPUT_DIR, EXPERIMENT_DIR, case_idx=0)\n",
        "else:\n",
        "    print(\"❌ No prediction files found for visualization\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 🧹 6. Cleanup and Summary\n",
        "\n",
        "Final cleanup and summary of the baseline test results.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Experiment Summary and Cleanup\n",
        "print(\"🎯 Baseline Test Summary\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Calculate total experiment time\n",
        "experiment_end_time = time.time()\n",
        "total_time = (experiment_end_time - time.time()) / 60  # This will be updated when run\n",
        "\n",
        "print(f\"📊 Dataset Information:\")\n",
        "print(f\"   Dataset ID: {DATASET_ID}\")\n",
        "print(f\"   Dataset Name: {DATASET_NAME}\")\n",
        "print(f\"   Training Cases: 15\")\n",
        "print(f\"   Test Cases: 3\")\n",
        "print(f\"   Image Dimensions: 96x96x48\")\n",
        "\n",
        "print(f\"\\n🏋️ Training Information:\")\n",
        "print(f\"   Configuration: 2D\")\n",
        "print(f\"   Fold: 0 (single fold)\")\n",
        "print(f\"   Model Type: nnUNetTrainer\")\n",
        "\n",
        "print(f\"\\n🔮 Inference Information:\")\n",
        "print(f\"   Test Cases Processed: {len(list(OUTPUT_DIR.glob('*.nii.gz')))}\")\n",
        "print(f\"   Prediction Files Generated: {len(list(OUTPUT_DIR.glob('*.nii.gz')))}\")\n",
        "\n",
        "print(f\"\\n📁 Generated Files:\")\n",
        "print(f\"   Experiment Directory: {EXPERIMENT_DIR}\")\n",
        "print(f\"   Training Log: {EXPERIMENT_DIR}/training_log.txt\")\n",
        "print(f\"   Results Summary: {EXPERIMENT_DIR}/inference_results.json\")\n",
        "print(f\"   Visualizations: {len(list(EXPERIMENT_DIR.glob('*.png')))} PNG files\")\n",
        "\n",
        "# List all generated files\n",
        "print(f\"\\n📋 All Experiment Files:\")\n",
        "for file_path in sorted(EXPERIMENT_DIR.rglob(\"*\")):\n",
        "    if file_path.is_file():\n",
        "        size = file_path.stat().st_size / (1024*1024)  # MB\n",
        "        print(f\"   {file_path.relative_to(EXPERIMENT_DIR)} ({size:.2f} MB)\")\n",
        "\n",
        "print(f\"\\n💡 Next Steps:\")\n",
        "print(f\"   1. Review training log: {EXPERIMENT_DIR}/training_log.txt\")\n",
        "print(f\"   2. Analyze predictions in: {OUTPUT_DIR}\")\n",
        "print(f\"   3. For better results, try 3D training: nnUNetv2_train {DATASET_ID} 3d_fullres 0\")\n",
        "print(f\"   4. For cross-validation, train all 5 folds\")\n",
        "print(f\"   5. Use real medical data for production experiments\")\n",
        "\n",
        "print(f\"\\n🔧 Cleanup Options:\")\n",
        "print(f\"   - Keep experiment files: {EXPERIMENT_DIR}\")\n",
        "print(f\"   - Remove large training files to save space\")\n",
        "print(f\"   - Archive results for future reference\")\n",
        "\n",
        "# Optional cleanup (commented out by default)\n",
        "# print(f\"\\n🧹 Cleaning up temporary files...\")\n",
        "# if EXPERIMENT_DIR.exists():\n",
        "#     for temp_file in EXPERIMENT_DIR.glob(\"*.tmp\"):\n",
        "#         temp_file.unlink()\n",
        "#     print(\"✅ Temporary files cleaned up\")\n",
        "\n",
        "print(f\"\\n🎉 Baseline test completed successfully!\")\n",
        "print(f\"📁 All results saved in: {EXPERIMENT_DIR}\")\n",
        "\n",
        "# Save experiment metadata\n",
        "experiment_metadata = {\n",
        "    \"experiment_id\": EXPERIMENT_ID,\n",
        "    \"dataset_id\": DATASET_ID,\n",
        "    \"dataset_name\": DATASET_NAME,\n",
        "    \"configuration\": \"2d\",\n",
        "    \"fold\": 0,\n",
        "    \"num_training_cases\": 15,\n",
        "    \"num_test_cases\": 3,\n",
        "    \"image_dimensions\": [96, 96, 48],\n",
        "    \"experiment_directory\": str(EXPERIMENT_DIR),\n",
        "    \"timestamp\": datetime.now().isoformat(),\n",
        "    \"status\": \"completed\"\n",
        "}\n",
        "\n",
        "metadata_file = EXPERIMENT_DIR / \"experiment_metadata.json\"\n",
        "with open(metadata_file, 'w') as f:\n",
        "    json.dump(experiment_metadata, f, indent=2)\n",
        "\n",
        "print(f\"💾 Experiment metadata saved to: {metadata_file}\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
