{"cells":[{"cell_type":"markdown","source":["# nnU-Net Dataset Download and Setup\n","\n","This notebook downloads and prepares a dataset for nnU-Net baseline experiments.\n","\n","**What this notebook does:**\n","1. Sets up nnU-Net environment variables\n","2. Downloads Medical Segmentation Decathlon Task02_Heart dataset (or creates synthetic data)\n","3. Converts to nnU-Net format\n","4. Runs preprocessing\n","5. Verifies setup is ready for training"],"metadata":{"id":"4S1cXgItpvuJ"}},{"cell_type":"code","source":["# If running in Google Colab, mount the drive\n","# Ref: https://netraneupane.medium.com/how-to-install-libraries-permanently-in-google-colab-fb15a585d8a5\n","DRIVE_ROOT_PATH = '/content/drive/MyDrive'\n","RLCCSAM_ROOT_PATH = f\"{DRIVE_ROOT_PATH}/RL-CC-SAM\"\n","\n","from google.colab import drive\n","drive.mount(\"/content/drive\")\n","\n","## And use the environment\n","!source {DRIVE_ROOT_PATH}/colab_envs/llms/bin/activate;"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W5w3i6A5p53v","executionInfo":{"status":"ok","timestamp":1749975311939,"user_tz":420,"elapsed":20408,"user":{"displayName":"Chiu Don","userId":"10237937896367434430"}},"outputId":"b737c2ba-0ecc-42be-c05d-afe1d0c9770e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["## ğŸ“¦ Install Dependencies\n","\n","!pip install requests tqdm nibabel numpy"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I1JgN0KOqEYt","executionInfo":{"status":"ok","timestamp":1749975333309,"user_tz":420,"elapsed":9594,"user":{"displayName":"Chiu Don","userId":"10237937896367434430"}},"outputId":"79ec514b-8fc1-4afc-b8f8-888e09bd7967"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n","Requirement already satisfied: nibabel in /usr/local/lib/python3.11/dist-packages (5.3.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.4.26)\n","Requirement already satisfied: importlib-resources>=5.12 in /usr/local/lib/python3.11/dist-packages (from nibabel) (6.5.2)\n","Requirement already satisfied: packaging>=20 in /usr/local/lib/python3.11/dist-packages (from nibabel) (24.2)\n","Requirement already satisfied: typing-extensions>=4.6 in /usr/local/lib/python3.11/dist-packages (from nibabel) (4.14.0)\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Qr4oN_Yipho8","executionInfo":{"status":"ok","timestamp":1749975333629,"user_tz":420,"elapsed":317,"user":{"displayName":"Chiu Don","userId":"10237937896367434430"}},"outputId":"03ee698d-bb89-4df8-f72e-6e0138b85c18"},"outputs":[{"output_type":"stream","name":"stdout","text":["âœ… Dependencies installed and imported\n"]}],"source":["# Import libraries\n","import os\n","import sys\n","import subprocess\n","import tarfile\n","import shutil\n","import requests\n","from pathlib import Path\n","import tempfile\n","import json\n","import numpy as np\n","from tqdm import tqdm\n","\n","print(\"âœ… Dependencies installed and imported\")\n"]},{"cell_type":"markdown","source":["## ğŸŒ Setup Environment Variables\n"],"metadata":{"id":"UiqPcRCfqNGz"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"9LyMfVPUpho9"},"outputs":[],"source":["# Set up nnU-Net environment variables\n","datasets_path = f\"{RLCCSAM_ROOT_PATH}/datasets\"\n","\n","# Set environment variables\n","env_vars = {\n","    'nnUNet_raw': str(datasets_path / \"nnUNet_raw\"),\n","    'nnUNet_preprocessed': str(datasets_path / \"nnUNet_preprocessed\"),\n","    'nnUNet_results': str(datasets_path / \"nnUNet_results\")\n","}\n","\n","for key, value in env_vars.items():\n","    os.environ[key] = value\n","    Path(value).mkdir(parents=True, exist_ok=True)\n","    print(f\"âœ… Set {key} = {value}\")\n","\n","print(\"\\nğŸ” Environment verification:\")\n","for key in env_vars.keys():\n","    print(f\"   {key}: {os.environ[key]}\")\n"]},{"cell_type":"markdown","source":["## ğŸ“¥ Download Dataset\n","\n","We'll try to download the MSD Heart dataset first. If that fails, we'll create a synthetic dataset for testing.\n"],"metadata":{"id":"TAggIUnwrcnk"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"-dgwb2d6pho9"},"outputs":[],"source":["def download_file_with_progress(url, destination):\n","    \"\"\"Download a file with progress bar.\"\"\"\n","    try:\n","        response = requests.get(url, stream=True)\n","        response.raise_for_status()\n","        total_size = int(response.headers.get('content-length', 0))\n","\n","        with open(destination, 'wb') as file, tqdm(\n","            desc=Path(destination).name,\n","            total=total_size,\n","            unit='B',\n","            unit_scale=True,\n","            unit_divisor=1024,\n","        ) as pbar:\n","            for chunk in response.iter_content(chunk_size=8192):\n","                if chunk:\n","                    file.write(chunk)\n","                    pbar.update(len(chunk))\n","        return True\n","    except Exception as e:\n","        print(f\"âŒ Download failed: {e}\")\n","        return False\n","\n","# Try to download MSD Heart dataset\n","download_dir = Path(tempfile.mkdtemp(prefix=\"nnunet_download_\"))\n","print(f\"ğŸ“ Using temporary directory: {download_dir}\")\n","\n","# MSD download URLs (may need updating)\n","msd_urls = [\n","    \"http://medicaldecathlon.com/files/Task02_Heart.tar\",\n","]\n","\n","tar_file = download_dir / \"Task02_Heart.tar\"\n","downloaded = False\n","\n","print(\"ğŸ“¥ Attempting to download MSD Task02_Heart...\")\n","for url in msd_urls:\n","    print(f\"ğŸŒ Trying: {url}\")\n","    if download_file_with_progress(url, tar_file):\n","        downloaded = True\n","        print(\"âœ… Download successful!\")\n","        break\n","\n","if not downloaded:\n","    print(\"âš ï¸ MSD download failed. Will create synthetic dataset instead.\")\n","else:\n","    print(f\"ğŸ“¦ Downloaded to: {tar_file}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LrP5JAFrphpA"},"outputs":[],"source":["def extract_tar_file(tar_path, destination_path, verbose=True):\n","    \"\"\"\n","    Extract a tar file to a specified destination path.\n","\n","    Parameters:\n","    -----------\n","    tar_path : str or Path\n","        Path to the tar file to extract\n","    destination_path : str or Path\n","        Directory where the tar file contents will be extracted\n","    verbose : bool\n","        Whether to print progress information\n","\n","    Returns:\n","    --------\n","    Path or None\n","        Path to the extracted directory if successful, None if failed\n","\n","    Example:\n","    --------\n","    # Extract a downloaded MSD dataset\n","    tar_file = \"/path/to/Task02_Heart.tar\"\n","    extract_dir = \"/path/to/extract/location\"\n","    extracted_folder = extract_tar_file(tar_file, extract_dir)\n","\n","    if extracted_folder:\n","        print(f\"Successfully extracted to: {extracted_folder}\")\n","    \"\"\"\n","    import tarfile\n","    from pathlib import Path\n","\n","    tar_path = Path(tar_path)\n","    destination_path = Path(destination_path)\n","\n","    # Validate inputs\n","    if not tar_path.exists():\n","        print(f\"âŒ Error: Tar file not found: {tar_path}\")\n","        return None\n","\n","    if not tar_path.suffix in ['.tar', '.gz', '.bz2', '.xz'] and not str(tar_path).endswith('.tar.gz'):\n","        print(f\"âš ï¸  Warning: File doesn't appear to be a tar archive: {tar_path}\")\n","\n","    # Create destination directory if it doesn't exist\n","    destination_path.mkdir(parents=True, exist_ok=True)\n","\n","    try:\n","        if verbose:\n","            print(f\"ğŸ“‚ Extracting {tar_path.name} to {destination_path}\")\n","            print(f\"   Source: {tar_path}\")\n","            print(f\"   Destination: {destination_path}\")\n","\n","        # Open and extract the tar file\n","        with tarfile.open(tar_path, 'r') as tar:\n","            # Get list of members for progress\n","            members = tar.getmembers()\n","\n","            if verbose:\n","                print(f\"   ğŸ“Š Total files to extract: {len(members)}\")\n","\n","            # Extract all files\n","            tar.extractall(path=destination_path)\n","\n","        if verbose:\n","            print(\"âœ… Extraction completed successfully!\")\n","\n","        # Find the extracted folder(s)\n","        extracted_items = list(destination_path.iterdir())\n","\n","        # If there's exactly one directory, return it\n","        directories = [item for item in extracted_items if item.is_dir()]\n","        if len(directories) == 1:\n","            extracted_folder = directories[0]\n","            if verbose:\n","                print(f\"ğŸ“ Extracted folder: {extracted_folder}\")\n","            return extracted_folder\n","        elif len(directories) > 1:\n","            if verbose:\n","                print(f\"ğŸ“ Multiple directories extracted:\")\n","                for d in directories:\n","                    print(f\"   - {d}\")\n","            return directories[0]  # Return the first one\n","        else:\n","            # No directories, files were extracted directly\n","            if verbose:\n","                print(f\"ğŸ“„ Files extracted directly to: {destination_path}\")\n","            return destination_path\n","\n","    except tarfile.TarError as e:\n","        print(f\"âŒ Error extracting tar file: {e}\")\n","        return None\n","    except Exception as e:\n","        print(f\"âŒ Unexpected error during extraction: {e}\")\n","        return None\n","\n","# Example usage:\n","print(\"ğŸ”§ Tar extraction utility function defined!\")\n","print(\"ğŸ“‹ Usage example:\")\n","print(\"   extracted_path = extract_tar_file('/path/to/dataset.tar', '/path/to/extract/to')\")\n","print(\"   if extracted_path:\")\n","print(\"       print(f'Dataset extracted to: {extracted_path}')\")\n"]},{"cell_type":"code","source":["!tar -xf {DRIVE_ROOT_PATH}/datasets/MSD/Task02_Heart.tar -C {RLCCSAM_ROOT_PATH}/datasets/nnUNet_raw"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cCv0CXt7ssoN","executionInfo":{"status":"ok","timestamp":1749976204601,"user_tz":420,"elapsed":423,"user":{"displayName":"Chiu Don","userId":"10237937896367434430"}},"outputId":"a7697485-b144-4310-e2ce-25b441cd7752"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tar: /content/drive/MyDrive/RL-CC-SAM/MSD/datasets: Cannot open: No such file or directory\n","tar: Error is not recoverable: exiting now\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Io9zY4D4pho-"},"outputs":[],"source":["# Install nibabel for creating NIfTI files\n","%pip install nibabel\n","import nibabel as nib\n"]},{"cell_type":"markdown","source":["## ğŸ”¬ Create Synthetic Dataset (Fallback)\n","\n","If the MSD download failed, we'll create a small synthetic dataset for testing:\n"],"metadata":{"id":"stI-sytpqsIG"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"pfMbsOLDpho-"},"outputs":[],"source":["def create_synthetic_dataset(base_dir, num_cases=10):\n","    \"\"\"Create a synthetic dataset for testing nnU-Net.\"\"\"\n","\n","    print(f\"ğŸ”¬ Creating synthetic dataset with {num_cases} cases...\")\n","\n","    dataset_dir = base_dir / \"Dataset999_Synthetic\"\n","    images_tr = dataset_dir / \"imagesTr\"\n","    labels_tr = dataset_dir / \"labelsTr\"\n","\n","    # Create directories\n","    images_tr.mkdir(parents=True, exist_ok=True)\n","    labels_tr.mkdir(parents=True, exist_ok=True)\n","\n","    # Create synthetic data\n","    for i in tqdm(range(num_cases), desc=\"Creating cases\"):\n","        case_id = f\"case_{i:03d}\"\n","\n","        # Create 3D image (64x64x32 for quick processing)\n","        image_data = np.random.randint(0, 1000, (64, 64, 32), dtype=np.int16)\n","        # Add some structure (simulated \"organ\")\n","        image_data[20:40, 20:40, 10:20] += 200\n","\n","        # Create segmentation mask\n","        seg_data = np.zeros((64, 64, 32), dtype=np.uint8)\n","        seg_data[25:35, 25:35, 12:18] = 1  # \"organ\" label\n","\n","        # Save as NIfTI files\n","        img_nifti = nib.Nifti1Image(image_data, affine=np.eye(4))\n","        seg_nifti = nib.Nifti1Image(seg_data, affine=np.eye(4))\n","\n","        nib.save(img_nifti, images_tr / f\"{case_id}_0000.nii.gz\")\n","        nib.save(seg_nifti, labels_tr / f\"{case_id}.nii.gz\")\n","\n","    # Create dataset.json\n","    dataset_json = {\n","        \"channel_names\": {\"0\": \"synthetic\"},\n","        \"labels\": {\"background\": 0, \"organ\": 1},\n","        \"numTraining\": num_cases,\n","        \"file_ending\": \".nii.gz\"\n","    }\n","\n","    with open(dataset_dir / \"dataset.json\", 'w') as f:\n","        json.dump(dataset_json, f, indent=2)\n","\n","    print(f\"âœ… Created synthetic dataset at: {dataset_dir}\")\n","    return dataset_dir\n","\n","# Create synthetic dataset if needed\n","if not downloaded:\n","    dataset_path = create_synthetic_dataset(download_dir)\n","    dataset_id = 999\n","    print(f\"ğŸ“Š Synthetic dataset ready at: {dataset_path}\")\n","else:\n","    # Extract the downloaded MSD dataset\n","    print(\"ğŸ“‚ Extracting MSD dataset...\")\n","    with tarfile.open(tar_file, 'r') as tar:\n","        tar.extractall(download_dir)\n","\n","    # Find extracted folder\n","    extracted_folders = [d for d in download_dir.iterdir() if d.is_dir() and 'Task02' in d.name]\n","    if extracted_folders:\n","        dataset_path = extracted_folders[0]\n","        dataset_id = 2\n","        print(f\"âœ… Extracted MSD dataset at: {dataset_path}\")\n","    else:\n","        print(\"âš ï¸ Extraction failed, creating synthetic dataset instead\")\n","        dataset_path = create_synthetic_dataset(download_dir)\n","        dataset_id = 999\n"]},{"cell_type":"markdown","source":["## ğŸ”„ Convert to nnU-Net Format\n"],"metadata":{"id":"Lx8tvOZGqvtr"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"qDbYqbj9pho-"},"outputs":[],"source":["def convert_to_nnunet_format(dataset_path, target_id):\n","    \"\"\"Convert dataset to nnU-Net format.\"\"\"\n","    print(f\"ğŸ”„ Converting dataset to nnU-Net format (ID: {target_id})...\")\n","\n","    raw_data_folder = Path(os.environ['nnUNet_raw'])\n","\n","    if 'Task02_Heart' in str(dataset_path):\n","        # Use MSD converter for official MSD data\n","        cmd = [\n","            'nnUNetv2_convert_MSD_dataset',\n","            '-i', str(dataset_path),\n","            '-overwrite_id', str(target_id)\n","        ]\n","\n","        try:\n","            result = subprocess.run(cmd, check=True, capture_output=True, text=True)\n","            print(\"âœ… MSD conversion successful\")\n","            return target_id\n","        except subprocess.CalledProcessError as e:\n","            print(f\"âŒ MSD conversion failed: {e}\")\n","            print(f\"Error output: {e.stderr}\")\n","            return None\n","    else:\n","        # For synthetic datasets, copy directly\n","        target_name = f\"Dataset{target_id:03d}_Synthetic\"\n","        target_path = raw_data_folder / target_name\n","\n","        if target_path.exists():\n","            shutil.rmtree(target_path)\n","\n","        shutil.copytree(dataset_path, target_path)\n","        print(f\"âœ… Copied synthetic dataset to: {target_path}\")\n","        return target_id\n","\n","# Convert dataset\n","converted_id = convert_to_nnunet_format(dataset_path, dataset_id)\n","\n","if converted_id:\n","    print(f\"ğŸ¯ Dataset successfully converted with ID: {converted_id}\")\n","else:\n","    print(\"âŒ Dataset conversion failed\")\n","    raise RuntimeError(\"Cannot proceed without successful conversion\")\n"]},{"cell_type":"markdown","source":["## âš™ï¸ Run nnU-Net Preprocessing\n"],"metadata":{"id":"_d0McWntrDMP"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"sqHSeWhtpho-"},"outputs":[],"source":["def run_preprocessing(dataset_id):\n","    \"\"\"Run nnU-Net preprocessing.\"\"\"\n","    print(f\"âš™ï¸ Running nnU-Net preprocessing for dataset {dataset_id}...\")\n","    print(\"This may take a few minutes...\")\n","\n","    cmd = [\n","        'nnUNetv2_plan_and_preprocess',\n","        '-d', str(dataset_id),\n","        '--verify_dataset_integrity'\n","    ]\n","\n","    try:\n","        # Run preprocessing (this may take a while)\n","        result = subprocess.run(cmd, check=True, capture_output=True, text=True)\n","        print(\"âœ… Preprocessing completed successfully!\")\n","        print(\"ğŸ” Dataset integrity verified\")\n","\n","        # Show some output\n","        if result.stdout:\n","            print(\"\\nğŸ“‹ Preprocessing summary:\")\n","            # Show last few lines of output\n","            lines = result.stdout.strip().split('\\n')\n","            for line in lines[-5:]:\n","                if line.strip():\n","                    print(f\"   {line}\")\n","\n","        return True\n","    except subprocess.CalledProcessError as e:\n","        print(f\"âŒ Preprocessing failed: {e}\")\n","        print(f\"Error output: {e.stderr}\")\n","        return False\n","\n","# Run preprocessing\n","preprocessing_success = run_preprocessing(converted_id)\n","\n","if preprocessing_success:\n","    print(\"ğŸ‰ Preprocessing completed successfully!\")\n","else:\n","    print(\"âš ï¸ Preprocessing had issues, but we can continue\")\n"]},{"cell_type":"markdown","source":["## ğŸ” Verify Setup\n"],"metadata":{"id":"7vxC2Al0rG4g"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"7yiHWYXPpho_"},"outputs":[],"source":["def verify_setup(dataset_id):\n","    \"\"\"Verify that everything is set up correctly.\"\"\"\n","    print(\"ğŸ” Verifying setup...\\n\")\n","\n","    # Check environment variables\n","    print(\"ğŸ“ Environment Variables:\")\n","    required_vars = ['nnUNet_raw', 'nnUNet_preprocessed', 'nnUNet_results']\n","    all_vars_ok = True\n","\n","    for var in required_vars:\n","        if var in os.environ:\n","            print(f\"   âœ… {var}: {os.environ[var]}\")\n","        else:\n","            print(f\"   âŒ {var}: Not set\")\n","            all_vars_ok = False\n","\n","    # Check dataset files\n","    print(f\"\\nğŸ“ Dataset Files (ID: {dataset_id}):\")\n","\n","    raw_path = Path(os.environ['nnUNet_raw'])\n","    preprocessed_path = Path(os.environ['nnUNet_preprocessed'])\n","\n","    # Find dataset folders\n","    raw_datasets = list(raw_path.glob(f\"Dataset{dataset_id:03d}_*\"))\n","    preprocessed_datasets = list(preprocessed_path.glob(f\"Dataset{dataset_id:03d}_*\"))\n","\n","    datasets_ok = True\n","\n","    if raw_datasets:\n","        raw_dataset = raw_datasets[0]\n","        print(f\"   âœ… Raw dataset: {raw_dataset}\")\n","\n","        # Check contents\n","        images_tr = raw_dataset / \"imagesTr\"\n","        labels_tr = raw_dataset / \"labelsTr\"\n","        dataset_json = raw_dataset / \"dataset.json\"\n","\n","        if images_tr.exists():\n","            num_images = len(list(images_tr.glob(\"*.nii.gz\")))\n","            print(f\"      ğŸ“Š Training images: {num_images}\")\n","\n","        if labels_tr.exists():\n","            num_labels = len(list(labels_tr.glob(\"*.nii.gz\")))\n","            print(f\"      ğŸ·ï¸  Training labels: {num_labels}\")\n","\n","        if dataset_json.exists():\n","            print(f\"      ğŸ“‹ Dataset.json: âœ…\")\n","    else:\n","        print(f\"   âŒ Raw dataset not found at: {raw_path / f'Dataset{dataset_id:03d}_*'}\")\n","        datasets_ok = False\n","\n","    if preprocessed_datasets:\n","        preprocessed_dataset = preprocessed_datasets[0]\n","        print(f\"   âœ… Preprocessed dataset: {preprocessed_dataset}\")\n","\n","        # Check for plans file\n","        plans_files = list(preprocessed_dataset.glob(\"*plans*.json\"))\n","        if plans_files:\n","            print(f\"      ğŸ“‹ Plans file: {plans_files[0].name}\")\n","    else:\n","        print(f\"   âŒ Preprocessed dataset not found at: {preprocessed_path / f'Dataset{dataset_id:03d}_*'}\")\n","        datasets_ok = False\n","\n","    return all_vars_ok and datasets_ok\n","\n","# Verify setup\n","setup_ok = verify_setup(converted_id)\n","\n","if setup_ok:\n","    print(\"\\nğŸ‰ Setup verification successful! You're ready to train!\")\n","else:\n","    print(\"\\nâš ï¸ Some issues found, but you may still be able to proceed\")\n"]},{"cell_type":"markdown","source":["## ğŸ¯ Training Instructions\n","\n","Your dataset is now ready for training! Here are the commands you can use:\n"],"metadata":{"id":"bsqRth5vrK65"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"it5xmndypho_"},"outputs":[],"source":["def print_training_instructions(dataset_id):\n","    \"\"\"Print training instructions.\"\"\"\n","    print(\"\\n\" + \"=\"*60)\n","    print(\"ğŸ¯ TRAINING INSTRUCTIONS\")\n","    print(\"=\"*60)\n","    print(f\"Your dataset (ID: {dataset_id}) is ready for training!\\n\")\n","\n","    print(\"ğŸ“‹ Basic Training Commands:\")\n","    print(f\"   # Quick 2D training (faster, good for testing)\")\n","    print(f\"   !nnUNetv2_train {dataset_id} 2d 0\")\n","    print(f\"\")\n","    print(f\"   # 3D training (slower, usually better results)\")\n","    print(f\"   !nnUNetv2_train {dataset_id} 3d_fullres 0\")\n","\n","    print(\"\\nğŸ” Advanced Commands:\")\n","    print(f\"   # Find best configuration after training multiple models\")\n","    print(f\"   !nnUNetv2_find_best_configuration {dataset_id}\")\n","\n","    print(f\"\\n   # Run inference on new data\")\n","    print(f\"   !nnUNetv2_predict -i INPUT_FOLDER -o OUTPUT_FOLDER -d {dataset_id} -c 2d -f 0\")\n","\n","    print(\"\\nğŸ“š Helpful Commands:\")\n","    print(\"   !nnUNetv2_train -h                    # Training help\")\n","    print(\"   !nnUNetv2_predict -h                 # Prediction help\")\n","    print(f\"   !ls $nnUNet_preprocessed/Dataset{dataset_id:03d}_*/     # Check preprocessed data\")\n","\n","    print(\"\\nğŸ’¡ Tips:\")\n","    print(\"   - Start with 2d training for quick testing\")\n","    print(\"   - 3d_fullres usually gives better results for 3D data\")\n","    print(\"   - Use fold 0 for quick testing\")\n","    print(\"   - For full cross-validation, train folds 0,1,2,3,4\")\n","    print(\"   - Training time depends on dataset size and hardware\")\n","\n","    return True\n","\n","print_training_instructions(converted_id)\n"]},{"cell_type":"markdown","source":["## ğŸ§¹ Cleanup\n"],"metadata":{"id":"wW_1LwSorNM1"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"6f9QZ6Xqpho_"},"outputs":[],"source":["# Clean up temporary directory\n","if 'download_dir' in locals() and download_dir.exists():\n","    print(f\"ğŸ§¹ Cleaning up temporary directory: {download_dir}\")\n","    shutil.rmtree(download_dir, ignore_errors=True)\n","    print(\"âœ… Cleanup completed\")\n","\n","print(\"\\nğŸ‰ Dataset setup completed successfully!\")\n","print(f\"ğŸ“Š Dataset ID: {converted_id}\")\n","print(\"ğŸš€ You can now start training your nnU-Net models!\")\n"]},{"cell_type":"markdown","source":["## ğŸš€ Quick Start Training\n","\n","Run this cell to start a quick 2D training session:\n"],"metadata":{"id":"UqQuGqPOrPvk"}},{"cell_type":"markdown","source":["## ğŸ—‚ï¸ Utility Functions\n","\n","Additional utility functions for manual dataset handling:\n"],"metadata":{"id":"IfmbQOtyrSvt"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"GjCetlqlphpA"},"outputs":[],"source":["# Example: Extract a manually downloaded MSD dataset\n","# Uncomment and modify the paths below to use with your downloaded tar files\n","\n","# Example paths - modify these to match your actual file locations\n","# downloaded_tar = \"/path/to/your/downloaded/Task02_Heart.tar\"\n","# extract_location = str(Path.cwd().parent / \"datasets\" / \"manual_extraction\")\n","\n","# Extract the tar file\n","# extracted_folder = extract_tar_file(downloaded_tar, extract_location)\n","\n","# if extracted_folder:\n","#     print(f\"\\nğŸ¯ Next step: Convert to nnU-Net format\")\n","#     print(f\"Run: !nnUNetv2_convert_MSD_dataset -i {extracted_folder}\")\n","# else:\n","#     print(\"âŒ Extraction failed. Check the file path and try again.\")\n","\n","print(\"ğŸ’¡ To use this function:\")\n","print(\"1. Download a dataset tar file manually\")\n","print(\"2. Uncomment the code above\")\n","print(\"3. Update the paths to match your downloaded file\")\n","print(\"4. Run this cell to extract the dataset\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3llGfgOJphpA"},"outputs":[],"source":["# Uncomment and run to start training\n","# This will train a 2D model on fold 0 (quick test)\n","\n","# !nnUNetv2_train {converted_id} 2d 0\n","\n","print(f\"To start training, uncomment the line above and run this cell.\")\n","print(f\"Or copy this command to a new cell: !nnUNetv2_train {converted_id} 2d 0\")\n"]}],"metadata":{"language_info":{"name":"python"},"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"nbformat":4,"nbformat_minor":0}