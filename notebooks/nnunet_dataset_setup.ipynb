{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# nnU-Net Dataset Download and Setup\n",
        "\n",
        "This notebook downloads and prepares a dataset for nnU-Net baseline experiments.\n",
        "\n",
        "**What this notebook does:**\n",
        "1. Sets up nnU-Net environment variables\n",
        "2. Downloads Medical Segmentation Decathlon Task02_Heart dataset (or creates synthetic data)\n",
        "3. Converts to nnU-Net format\n",
        "4. Runs preprocessing\n",
        "5. Verifies setup is ready for training\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# If running in Google Colab, mount the drive\n",
        "# Ref: https://netraneupane.medium.com/how-to-install-libraries-permanently-in-google-colab-fb15a585d8a5\n",
        "DRIVE_ROOT_PATH = '/content/drive/MyDrive'\n",
        "RLCCSAM_ROOT_PATH = f\"{DRIVE_ROOT_PATH}/RL-CC-SAM\"\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "## And use the environment\n",
        "!source {DRIVE_ROOT_PATH}/colab_envs/llms/bin/activate;"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## üì¶ Install Dependencies\n",
        "\n",
        "!pip install requests tqdm nibabel numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import os\n",
        "import sys\n",
        "import subprocess\n",
        "import tarfile\n",
        "import shutil\n",
        "import requests\n",
        "from pathlib import Path\n",
        "import tempfile\n",
        "import json\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "print(\"‚úÖ Dependencies installed and imported\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## üåç Setup Environment Variables\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set up nnU-Net environment variables\n",
        "notebook_dir = Path.cwd()\n",
        "project_root = notebook_dir.parent  # Assuming notebook is in notebooks/ folder\n",
        "datasets_path = project_root / \"datasets\"\n",
        "\n",
        "# Set environment variables\n",
        "env_vars = {\n",
        "    'nnUNet_raw': str(datasets_path / \"nnUNet_raw\"),\n",
        "    'nnUNet_preprocessed': str(datasets_path / \"nnUNet_preprocessed\"),\n",
        "    'nnUNet_results': str(datasets_path / \"nnUNet_results\")\n",
        "}\n",
        "\n",
        "for key, value in env_vars.items():\n",
        "    os.environ[key] = value\n",
        "    Path(value).mkdir(parents=True, exist_ok=True)\n",
        "    print(f\"‚úÖ Set {key} = {value}\")\n",
        "\n",
        "print(\"\\nüîç Environment verification:\")\n",
        "for key in env_vars.keys():\n",
        "    print(f\"   {key}: {os.environ[key]}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## üì• Download Dataset\n",
        "\n",
        "We'll try to download the MSD Heart dataset first. If that fails, we'll create a synthetic dataset for testing.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def download_file_with_progress(url, destination):\n",
        "    \"\"\"Download a file with progress bar.\"\"\"\n",
        "    try:\n",
        "        response = requests.get(url, stream=True)\n",
        "        response.raise_for_status()\n",
        "        total_size = int(response.headers.get('content-length', 0))\n",
        "        \n",
        "        with open(destination, 'wb') as file, tqdm(\n",
        "            desc=Path(destination).name,\n",
        "            total=total_size,\n",
        "            unit='B',\n",
        "            unit_scale=True,\n",
        "            unit_divisor=1024,\n",
        "        ) as pbar:\n",
        "            for chunk in response.iter_content(chunk_size=8192):\n",
        "                if chunk:\n",
        "                    file.write(chunk)\n",
        "                    pbar.update(len(chunk))\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Download failed: {e}\")\n",
        "        return False\n",
        "\n",
        "# Try to download MSD Heart dataset\n",
        "download_dir = Path(tempfile.mkdtemp(prefix=\"nnunet_download_\"))\n",
        "print(f\"üìÅ Using temporary directory: {download_dir}\")\n",
        "\n",
        "# MSD download URLs (may need updating)\n",
        "msd_urls = [\n",
        "    \"http://medicaldecathlon.com/files/Task02_Heart.tar\",\n",
        "]\n",
        "\n",
        "tar_file = download_dir / \"Task02_Heart.tar\"\n",
        "downloaded = False\n",
        "\n",
        "print(\"üì• Attempting to download MSD Task02_Heart...\")\n",
        "for url in msd_urls:\n",
        "    print(f\"üåê Trying: {url}\")\n",
        "    if download_file_with_progress(url, tar_file):\n",
        "        downloaded = True\n",
        "        print(\"‚úÖ Download successful!\")\n",
        "        break\n",
        "\n",
        "if not downloaded:\n",
        "    print(\"‚ö†Ô∏è MSD download failed. Will create synthetic dataset instead.\")\n",
        "else:\n",
        "    print(f\"üì¶ Downloaded to: {tar_file}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install nibabel for creating NIfTI files\n",
        "%pip install nibabel\n",
        "import nibabel as nib\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## üî¨ Create Synthetic Dataset (Fallback)\n",
        "\n",
        "If the MSD download failed, we'll create a small synthetic dataset for testing:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_synthetic_dataset(base_dir, num_cases=10):\n",
        "    \"\"\"Create a synthetic dataset for testing nnU-Net.\"\"\"\n",
        "    \n",
        "    print(f\"üî¨ Creating synthetic dataset with {num_cases} cases...\")\n",
        "    \n",
        "    dataset_dir = base_dir / \"Dataset999_Synthetic\"\n",
        "    images_tr = dataset_dir / \"imagesTr\"\n",
        "    labels_tr = dataset_dir / \"labelsTr\"\n",
        "    \n",
        "    # Create directories\n",
        "    images_tr.mkdir(parents=True, exist_ok=True)\n",
        "    labels_tr.mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "    # Create synthetic data\n",
        "    for i in tqdm(range(num_cases), desc=\"Creating cases\"):\n",
        "        case_id = f\"case_{i:03d}\"\n",
        "        \n",
        "        # Create 3D image (64x64x32 for quick processing)\n",
        "        image_data = np.random.randint(0, 1000, (64, 64, 32), dtype=np.int16)\n",
        "        # Add some structure (simulated \"organ\")\n",
        "        image_data[20:40, 20:40, 10:20] += 200\n",
        "        \n",
        "        # Create segmentation mask\n",
        "        seg_data = np.zeros((64, 64, 32), dtype=np.uint8)\n",
        "        seg_data[25:35, 25:35, 12:18] = 1  # \"organ\" label\n",
        "        \n",
        "        # Save as NIfTI files\n",
        "        img_nifti = nib.Nifti1Image(image_data, affine=np.eye(4))\n",
        "        seg_nifti = nib.Nifti1Image(seg_data, affine=np.eye(4))\n",
        "        \n",
        "        nib.save(img_nifti, images_tr / f\"{case_id}_0000.nii.gz\")\n",
        "        nib.save(seg_nifti, labels_tr / f\"{case_id}.nii.gz\")\n",
        "    \n",
        "    # Create dataset.json\n",
        "    dataset_json = {\n",
        "        \"channel_names\": {\"0\": \"synthetic\"},\n",
        "        \"labels\": {\"background\": 0, \"organ\": 1},\n",
        "        \"numTraining\": num_cases,\n",
        "        \"file_ending\": \".nii.gz\"\n",
        "    }\n",
        "    \n",
        "    with open(dataset_dir / \"dataset.json\", 'w') as f:\n",
        "        json.dump(dataset_json, f, indent=2)\n",
        "    \n",
        "    print(f\"‚úÖ Created synthetic dataset at: {dataset_dir}\")\n",
        "    return dataset_dir\n",
        "\n",
        "# Create synthetic dataset if needed\n",
        "if not downloaded:\n",
        "    dataset_path = create_synthetic_dataset(download_dir)\n",
        "    dataset_id = 999\n",
        "    print(f\"üìä Synthetic dataset ready at: {dataset_path}\")\n",
        "else:\n",
        "    # Extract the downloaded MSD dataset\n",
        "    print(\"üìÇ Extracting MSD dataset...\")\n",
        "    with tarfile.open(tar_file, 'r') as tar:\n",
        "        tar.extractall(download_dir)\n",
        "    \n",
        "    # Find extracted folder\n",
        "    extracted_folders = [d for d in download_dir.iterdir() if d.is_dir() and 'Task02' in d.name]\n",
        "    if extracted_folders:\n",
        "        dataset_path = extracted_folders[0]\n",
        "        dataset_id = 2\n",
        "        print(f\"‚úÖ Extracted MSD dataset at: {dataset_path}\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è Extraction failed, creating synthetic dataset instead\")\n",
        "        dataset_path = create_synthetic_dataset(download_dir)\n",
        "        dataset_id = 999\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## üîÑ Convert to nnU-Net Format\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def convert_to_nnunet_format(dataset_path, target_id):\n",
        "    \"\"\"Convert dataset to nnU-Net format.\"\"\"\n",
        "    print(f\"üîÑ Converting dataset to nnU-Net format (ID: {target_id})...\")\n",
        "    \n",
        "    raw_data_folder = Path(os.environ['nnUNet_raw'])\n",
        "    \n",
        "    if 'Task02_Heart' in str(dataset_path):\n",
        "        # Use MSD converter for official MSD data\n",
        "        cmd = [\n",
        "            'nnUNetv2_convert_MSD_dataset',\n",
        "            '-i', str(dataset_path),\n",
        "            '-overwrite_id', str(target_id)\n",
        "        ]\n",
        "        \n",
        "        try:\n",
        "            result = subprocess.run(cmd, check=True, capture_output=True, text=True)\n",
        "            print(\"‚úÖ MSD conversion successful\")\n",
        "            return target_id\n",
        "        except subprocess.CalledProcessError as e:\n",
        "            print(f\"‚ùå MSD conversion failed: {e}\")\n",
        "            print(f\"Error output: {e.stderr}\")\n",
        "            return None\n",
        "    else:\n",
        "        # For synthetic datasets, copy directly\n",
        "        target_name = f\"Dataset{target_id:03d}_Synthetic\"\n",
        "        target_path = raw_data_folder / target_name\n",
        "        \n",
        "        if target_path.exists():\n",
        "            shutil.rmtree(target_path)\n",
        "        \n",
        "        shutil.copytree(dataset_path, target_path)\n",
        "        print(f\"‚úÖ Copied synthetic dataset to: {target_path}\")\n",
        "        return target_id\n",
        "\n",
        "# Convert dataset\n",
        "converted_id = convert_to_nnunet_format(dataset_path, dataset_id)\n",
        "\n",
        "if converted_id:\n",
        "    print(f\"üéØ Dataset successfully converted with ID: {converted_id}\")\n",
        "else:\n",
        "    print(\"‚ùå Dataset conversion failed\")\n",
        "    raise RuntimeError(\"Cannot proceed without successful conversion\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## ‚öôÔ∏è Run nnU-Net Preprocessing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_preprocessing(dataset_id):\n",
        "    \"\"\"Run nnU-Net preprocessing.\"\"\"\n",
        "    print(f\"‚öôÔ∏è Running nnU-Net preprocessing for dataset {dataset_id}...\")\n",
        "    print(\"This may take a few minutes...\")\n",
        "    \n",
        "    cmd = [\n",
        "        'nnUNetv2_plan_and_preprocess',\n",
        "        '-d', str(dataset_id),\n",
        "        '--verify_dataset_integrity'\n",
        "    ]\n",
        "    \n",
        "    try:\n",
        "        # Run preprocessing (this may take a while)\n",
        "        result = subprocess.run(cmd, check=True, capture_output=True, text=True)\n",
        "        print(\"‚úÖ Preprocessing completed successfully!\")\n",
        "        print(\"üîç Dataset integrity verified\")\n",
        "        \n",
        "        # Show some output\n",
        "        if result.stdout:\n",
        "            print(\"\\nüìã Preprocessing summary:\")\n",
        "            # Show last few lines of output\n",
        "            lines = result.stdout.strip().split('\\n')\n",
        "            for line in lines[-5:]:\n",
        "                if line.strip():\n",
        "                    print(f\"   {line}\")\n",
        "        \n",
        "        return True\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"‚ùå Preprocessing failed: {e}\")\n",
        "        print(f\"Error output: {e.stderr}\")\n",
        "        return False\n",
        "\n",
        "# Run preprocessing\n",
        "preprocessing_success = run_preprocessing(converted_id)\n",
        "\n",
        "if preprocessing_success:\n",
        "    print(\"üéâ Preprocessing completed successfully!\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Preprocessing had issues, but we can continue\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## üîç Verify Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def verify_setup(dataset_id):\n",
        "    \"\"\"Verify that everything is set up correctly.\"\"\"\n",
        "    print(\"üîç Verifying setup...\\n\")\n",
        "    \n",
        "    # Check environment variables\n",
        "    print(\"üìç Environment Variables:\")\n",
        "    required_vars = ['nnUNet_raw', 'nnUNet_preprocessed', 'nnUNet_results']\n",
        "    all_vars_ok = True\n",
        "    \n",
        "    for var in required_vars:\n",
        "        if var in os.environ:\n",
        "            print(f\"   ‚úÖ {var}: {os.environ[var]}\")\n",
        "        else:\n",
        "            print(f\"   ‚ùå {var}: Not set\")\n",
        "            all_vars_ok = False\n",
        "    \n",
        "    # Check dataset files\n",
        "    print(f\"\\nüìÅ Dataset Files (ID: {dataset_id}):\")\n",
        "    \n",
        "    raw_path = Path(os.environ['nnUNet_raw'])\n",
        "    preprocessed_path = Path(os.environ['nnUNet_preprocessed'])\n",
        "    \n",
        "    # Find dataset folders\n",
        "    raw_datasets = list(raw_path.glob(f\"Dataset{dataset_id:03d}_*\"))\n",
        "    preprocessed_datasets = list(preprocessed_path.glob(f\"Dataset{dataset_id:03d}_*\"))\n",
        "    \n",
        "    datasets_ok = True\n",
        "    \n",
        "    if raw_datasets:\n",
        "        raw_dataset = raw_datasets[0]\n",
        "        print(f\"   ‚úÖ Raw dataset: {raw_dataset}\")\n",
        "        \n",
        "        # Check contents\n",
        "        images_tr = raw_dataset / \"imagesTr\"\n",
        "        labels_tr = raw_dataset / \"labelsTr\"\n",
        "        dataset_json = raw_dataset / \"dataset.json\"\n",
        "        \n",
        "        if images_tr.exists():\n",
        "            num_images = len(list(images_tr.glob(\"*.nii.gz\")))\n",
        "            print(f\"      üìä Training images: {num_images}\")\n",
        "        \n",
        "        if labels_tr.exists():\n",
        "            num_labels = len(list(labels_tr.glob(\"*.nii.gz\")))\n",
        "            print(f\"      üè∑Ô∏è  Training labels: {num_labels}\")\n",
        "            \n",
        "        if dataset_json.exists():\n",
        "            print(f\"      üìã Dataset.json: ‚úÖ\")\n",
        "    else:\n",
        "        print(f\"   ‚ùå Raw dataset not found at: {raw_path / f'Dataset{dataset_id:03d}_*'}\")\n",
        "        datasets_ok = False\n",
        "    \n",
        "    if preprocessed_datasets:\n",
        "        preprocessed_dataset = preprocessed_datasets[0]\n",
        "        print(f\"   ‚úÖ Preprocessed dataset: {preprocessed_dataset}\")\n",
        "        \n",
        "        # Check for plans file\n",
        "        plans_files = list(preprocessed_dataset.glob(\"*plans*.json\"))\n",
        "        if plans_files:\n",
        "            print(f\"      üìã Plans file: {plans_files[0].name}\")\n",
        "    else:\n",
        "        print(f\"   ‚ùå Preprocessed dataset not found at: {preprocessed_path / f'Dataset{dataset_id:03d}_*'}\")\n",
        "        datasets_ok = False\n",
        "    \n",
        "    return all_vars_ok and datasets_ok\n",
        "\n",
        "# Verify setup\n",
        "setup_ok = verify_setup(converted_id)\n",
        "\n",
        "if setup_ok:\n",
        "    print(\"\\nüéâ Setup verification successful! You're ready to train!\")\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è Some issues found, but you may still be able to proceed\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## üéØ Training Instructions\n",
        "\n",
        "Your dataset is now ready for training! Here are the commands you can use:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def print_training_instructions(dataset_id):\n",
        "    \"\"\"Print training instructions.\"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"üéØ TRAINING INSTRUCTIONS\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"Your dataset (ID: {dataset_id}) is ready for training!\\n\")\n",
        "    \n",
        "    print(\"üìã Basic Training Commands:\")\n",
        "    print(f\"   # Quick 2D training (faster, good for testing)\")\n",
        "    print(f\"   !nnUNetv2_train {dataset_id} 2d 0\")\n",
        "    print(f\"\")\n",
        "    print(f\"   # 3D training (slower, usually better results)\")\n",
        "    print(f\"   !nnUNetv2_train {dataset_id} 3d_fullres 0\")\n",
        "    \n",
        "    print(\"\\nüîç Advanced Commands:\")\n",
        "    print(f\"   # Find best configuration after training multiple models\")\n",
        "    print(f\"   !nnUNetv2_find_best_configuration {dataset_id}\")\n",
        "    \n",
        "    print(f\"\\n   # Run inference on new data\")\n",
        "    print(f\"   !nnUNetv2_predict -i INPUT_FOLDER -o OUTPUT_FOLDER -d {dataset_id} -c 2d -f 0\")\n",
        "    \n",
        "    print(\"\\nüìö Helpful Commands:\")\n",
        "    print(\"   !nnUNetv2_train -h                    # Training help\")\n",
        "    print(\"   !nnUNetv2_predict -h                 # Prediction help\")\n",
        "    print(f\"   !ls $nnUNet_preprocessed/Dataset{dataset_id:03d}_*/     # Check preprocessed data\")\n",
        "    \n",
        "    print(\"\\nüí° Tips:\")\n",
        "    print(\"   - Start with 2d training for quick testing\")\n",
        "    print(\"   - 3d_fullres usually gives better results for 3D data\")\n",
        "    print(\"   - Use fold 0 for quick testing\")\n",
        "    print(\"   - For full cross-validation, train folds 0,1,2,3,4\")\n",
        "    print(\"   - Training time depends on dataset size and hardware\")\n",
        "    \n",
        "    return True\n",
        "\n",
        "print_training_instructions(converted_id)\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## üßπ Cleanup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clean up temporary directory\n",
        "if 'download_dir' in locals() and download_dir.exists():\n",
        "    print(f\"üßπ Cleaning up temporary directory: {download_dir}\")\n",
        "    shutil.rmtree(download_dir, ignore_errors=True)\n",
        "    print(\"‚úÖ Cleanup completed\")\n",
        "\n",
        "print(\"\\nüéâ Dataset setup completed successfully!\")\n",
        "print(f\"üìä Dataset ID: {converted_id}\")\n",
        "print(\"üöÄ You can now start training your nnU-Net models!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## üöÄ Quick Start Training\n",
        "\n",
        "Run this cell to start a quick 2D training session:\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## üóÇÔ∏è Utility Functions\n",
        "\n",
        "Additional utility functions for manual dataset handling:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_tar_file(tar_path, destination_path, verbose=True):\n",
        "    \"\"\"\n",
        "    Extract a tar file to a specified destination path.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    tar_path : str or Path\n",
        "        Path to the tar file to extract\n",
        "    destination_path : str or Path  \n",
        "        Directory where the tar file contents will be extracted\n",
        "    verbose : bool\n",
        "        Whether to print progress information\n",
        "        \n",
        "    Returns:\n",
        "    --------\n",
        "    Path or None\n",
        "        Path to the extracted directory if successful, None if failed\n",
        "        \n",
        "    Example:\n",
        "    --------\n",
        "    # Extract a downloaded MSD dataset\n",
        "    tar_file = \"/path/to/Task02_Heart.tar\"\n",
        "    extract_dir = \"/path/to/extract/location\"\n",
        "    extracted_folder = extract_tar_file(tar_file, extract_dir)\n",
        "    \n",
        "    if extracted_folder:\n",
        "        print(f\"Successfully extracted to: {extracted_folder}\")\n",
        "    \"\"\"\n",
        "    import tarfile\n",
        "    from pathlib import Path\n",
        "    \n",
        "    tar_path = Path(tar_path)\n",
        "    destination_path = Path(destination_path)\n",
        "    \n",
        "    # Validate inputs\n",
        "    if not tar_path.exists():\n",
        "        print(f\"‚ùå Error: Tar file not found: {tar_path}\")\n",
        "        return None\n",
        "        \n",
        "    if not tar_path.suffix in ['.tar', '.gz', '.bz2', '.xz'] and not str(tar_path).endswith('.tar.gz'):\n",
        "        print(f\"‚ö†Ô∏è  Warning: File doesn't appear to be a tar archive: {tar_path}\")\n",
        "    \n",
        "    # Create destination directory if it doesn't exist\n",
        "    destination_path.mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "    try:\n",
        "        if verbose:\n",
        "            print(f\"üìÇ Extracting {tar_path.name} to {destination_path}\")\n",
        "            print(f\"   Source: {tar_path}\")\n",
        "            print(f\"   Destination: {destination_path}\")\n",
        "        \n",
        "        # Open and extract the tar file\n",
        "        with tarfile.open(tar_path, 'r') as tar:\n",
        "            # Get list of members for progress\n",
        "            members = tar.getmembers()\n",
        "            \n",
        "            if verbose:\n",
        "                print(f\"   üìä Total files to extract: {len(members)}\")\n",
        "            \n",
        "            # Extract all files\n",
        "            tar.extractall(path=destination_path)\n",
        "            \n",
        "        if verbose:\n",
        "            print(\"‚úÖ Extraction completed successfully!\")\n",
        "        \n",
        "        # Find the extracted folder(s)\n",
        "        extracted_items = list(destination_path.iterdir())\n",
        "        \n",
        "        # If there's exactly one directory, return it\n",
        "        directories = [item for item in extracted_items if item.is_dir()]\n",
        "        if len(directories) == 1:\n",
        "            extracted_folder = directories[0]\n",
        "            if verbose:\n",
        "                print(f\"üìÅ Extracted folder: {extracted_folder}\")\n",
        "            return extracted_folder\n",
        "        elif len(directories) > 1:\n",
        "            if verbose:\n",
        "                print(f\"üìÅ Multiple directories extracted:\")\n",
        "                for d in directories:\n",
        "                    print(f\"   - {d}\")\n",
        "            return directories[0]  # Return the first one\n",
        "        else:\n",
        "            # No directories, files were extracted directly\n",
        "            if verbose:\n",
        "                print(f\"üìÑ Files extracted directly to: {destination_path}\")\n",
        "            return destination_path\n",
        "            \n",
        "    except tarfile.TarError as e:\n",
        "        print(f\"‚ùå Error extracting tar file: {e}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Unexpected error during extraction: {e}\")\n",
        "        return None\n",
        "\n",
        "# Example usage:\n",
        "print(\"üîß Tar extraction utility function defined!\")\n",
        "print(\"üìã Usage example:\")\n",
        "print(\"   extracted_path = extract_tar_file('/path/to/dataset.tar', '/path/to/extract/to')\")\n",
        "print(\"   if extracted_path:\")\n",
        "print(\"       print(f'Dataset extracted to: {extracted_path}')\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Extract a manually downloaded MSD dataset\n",
        "# Uncomment and modify the paths below to use with your downloaded tar files\n",
        "\n",
        "# Example paths - modify these to match your actual file locations\n",
        "# downloaded_tar = \"/path/to/your/downloaded/Task02_Heart.tar\"\n",
        "# extract_location = str(Path.cwd().parent / \"datasets\" / \"manual_extraction\")\n",
        "\n",
        "# Extract the tar file\n",
        "# extracted_folder = extract_tar_file(downloaded_tar, extract_location)\n",
        "\n",
        "# if extracted_folder:\n",
        "#     print(f\"\\nüéØ Next step: Convert to nnU-Net format\")\n",
        "#     print(f\"Run: !nnUNetv2_convert_MSD_dataset -i {extracted_folder}\")\n",
        "# else:\n",
        "#     print(\"‚ùå Extraction failed. Check the file path and try again.\")\n",
        "\n",
        "print(\"üí° To use this function:\")\n",
        "print(\"1. Download a dataset tar file manually\")\n",
        "print(\"2. Uncomment the code above\")\n",
        "print(\"3. Update the paths to match your downloaded file\")\n",
        "print(\"4. Run this cell to extract the dataset\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Uncomment and run to start training\n",
        "# This will train a 2D model on fold 0 (quick test)\n",
        "\n",
        "# !nnUNetv2_train {converted_id} 2d 0\n",
        "\n",
        "print(f\"To start training, uncomment the line above and run this cell.\")\n",
        "print(f\"Or copy this command to a new cell: !nnUNetv2_train {converted_id} 2d 0\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
